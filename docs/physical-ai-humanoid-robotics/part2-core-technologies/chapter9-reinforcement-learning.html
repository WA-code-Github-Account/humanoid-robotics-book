<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 9: Reinforcement Learning | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="humanoid robotics, physical AI, robotics book, AI systems, GIAIC"><meta data-rh="true" name="author" content="Aziza Siddiqui"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 9: Reinforcement Learning | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/humanoid-robotics-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning"><link data-rh="true" rel="alternate" href="https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning" hreflang="en"><link data-rh="true" rel="alternate" href="https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"PART 2: Core Technologies and Implementation","item":"https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/"},{"@type":"ListItem","position":2,"name":"Chapter 9: Reinforcement Learning for Robots","item":"https://A-Siddiqui-coder.github.io/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning/"}]}</script><link rel="stylesheet" href="/humanoid-robotics-book/assets/css/styles.90691243.css">
<script src="/humanoid-robotics-book/assets/js/runtime~main.36bbae81.js" defer="defer"></script>
<script src="/humanoid-robotics-book/assets/js/main.1179bc64.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/humanoid-robotics-book/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/humanoid-robotics-book/"><div class="navbar__logo"><img src="/humanoid-robotics-book/img/logo.svg" alt="Robotics Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/humanoid-robotics-book/img/logo.svg" alt="Robotics Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/intro">ðŸ“š Book</a><a class="navbar__item navbar__link" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part1-foundations/intro">Part 1</a><a class="navbar__item navbar__link" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/intro">Part 2</a><a class="navbar__item navbar__link" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part3-advanced-topics/intro">Part 3</a><a class="navbar__item navbar__link" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part4-interdisciplinary-frontiers/intro">Part 4</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/A-Siddiqui-coder/humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/intro"><span title="Physical AI and Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI and Humanoid Robotics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/intro"><span title="Introduction to the Course" class="linkLabel_WmDU">Introduction to the Course</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part1-foundations"><span title="PART 1: Foundations" class="categoryLinkLabel_W154">PART 1: Foundations</span></a><button aria-label="Expand sidebar category &#x27;PART 1: Foundations&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies"><span title="PART 2: Core Technologies and Implementation" class="categoryLinkLabel_W154">PART 2: Core Technologies and Implementation</span></a><button aria-label="Collapse sidebar category &#x27;PART 2: Core Technologies and Implementation&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter6-sensors-perception"><span title="Chapter 6: Sensors and Perception" class="categoryLinkLabel_W154">Chapter 6: Sensors and Perception</span></a><button aria-label="Expand sidebar category &#x27;Chapter 6: Sensors and Perception&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision"><span title="Chapter 7: Computer Vision for Robotics" class="categoryLinkLabel_W154">Chapter 7: Computer Vision for Robotics</span></a><button aria-label="Expand sidebar category &#x27;Chapter 7: Computer Vision for Robotics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter8-machine-learning"><span title="Chapter 8: Machine Learning in Robotics" class="categoryLinkLabel_W154">Chapter 8: Machine Learning in Robotics</span></a><button aria-label="Expand sidebar category &#x27;Chapter 8: Machine Learning in Robotics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning"><span title="Chapter 9: Reinforcement Learning for Robots" class="linkLabel_WmDU">Chapter 9: Reinforcement Learning for Robots</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter10-ros"><span title="Chapter 10: ROS (Robot Operating System)" class="categoryLinkLabel_W154">Chapter 10: ROS (Robot Operating System)</span></a><button aria-label="Expand sidebar category &#x27;Chapter 10: ROS (Robot Operating System)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part3-advanced-topics"><span title="PART 3: Advanced Topics and Humanoid Robotics" class="categoryLinkLabel_W154">PART 3: Advanced Topics and Humanoid Robotics</span></a><button aria-label="Expand sidebar category &#x27;PART 3: Advanced Topics and Humanoid Robotics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part4-interdisciplinary-frontiers"><span title="Part 4: Interdisciplinary Frontiers" class="categoryLinkLabel_W154">Part 4: Interdisciplinary Frontiers</span></a><button aria-label="Expand sidebar category &#x27;Part 4: Interdisciplinary Frontiers&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/humanoid-robotics-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Physical AI and Humanoid Robotics</span></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies"><span>PART 2: Core Technologies and Implementation</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 9: Reinforcement Learning for Robots</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 9: Reinforcement Learning</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">â€‹</a></h2>
<p><strong>Reinforcement Learning (RL)</strong> is a paradigm of machine learning where an &quot;agent&quot; learns to make decisions by performing actions in an &quot;environment&quot; to maximize a cumulative &quot;reward.&quot; It is fundamentally about learning from trial and error. For robotics, RL holds immense promise. It is a framework that allows a robot to autonomously discover how to perform complex tasks, like walking, running, and manipulating objects, that are extremely difficult to program by hand.</p>
<p>Instead of being told <em>what</em> to do, the robot is simply given a goal in the form of a reward signal and then discovers for itself <em>how</em> to achieve that goal. This chapter explores the core concepts of RL and the algorithms that are enabling breakthroughs in robotic control.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="91-the-language-of-reinforcement-learning">9.1 The Language of Reinforcement Learning<a href="#91-the-language-of-reinforcement-learning" class="hash-link" aria-label="Direct link to 9.1 The Language of Reinforcement Learning" title="Direct link to 9.1 The Language of Reinforcement Learning" translate="no">â€‹</a></h2>
<p>RL has its own specific vocabulary to describe the learning problem:</p>
<ul>
<li class=""><strong>Agent:</strong> The learner and decision-maker. In our case, this is the humanoid robot or, more specifically, the control policy running on its computer.</li>
<li class=""><strong>Environment:</strong> The world in which the agent exists and interacts. This includes the laws of physics, the objects in the world, and the robot&#x27;s own body.</li>
<li class=""><strong>State (S):</strong> A complete description of the agent and environment at a single point in time. For a humanoid, the state might include all its joint angles and velocities, its orientation from the IMU, and data from its cameras and other sensors.</li>
<li class=""><strong>Action (A):</strong> A choice the agent can make. For a robot, an action is typically the command sent to the motors, such as a vector of desired joint torques or target positions.</li>
<li class=""><strong>Reward (R):</strong> A scalar feedback signal that the agent receives from the environment after taking an action in a state. The reward tells the agent how good or bad that action was with respect to the overall goal.</li>
<li class=""><strong>Policy (Ï€):</strong> The &quot;brain&quot; of the agent. The policy is a function (or mapping) that takes a state as input and produces an action as output. The entire goal of reinforcement learning is to find the optimal policy that maximizes the total expected reward over time.</li>
</ul>
<p>The process is a continuous loop: the agent observes the current <strong>state</strong>, the <strong>policy</strong> chooses an <strong>action</strong>, the agent performs the action and receives a <strong>reward</strong>, and the environment transitions to a new state. An <strong>episode</strong> is a full sequence of these steps from a starting state to a terminal state (e.g., the robot falls or completes the task).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="92-exploration-vs-exploitation">9.2 Exploration vs. Exploitation<a href="#92-exploration-vs-exploitation" class="hash-link" aria-label="Direct link to 9.2 Exploration vs. Exploitation" title="Direct link to 9.2 Exploration vs. Exploitation" translate="no">â€‹</a></h2>
<p>The agent faces a fundamental dilemma at every step: should it <strong>exploit</strong> its current knowledge by taking the action that it currently believes will yield the highest reward? Or should it <strong>explore</strong> by trying a different, perhaps random, action that might lead to the discovery of an even better reward in the future? An agent that only ever exploits might get stuck in a suboptimal strategy, while an agent that only ever explores will never settle on a good solution. A critical function of any RL algorithm is to effectively balance this trade-off.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="93-major-families-of-rl-algorithms">9.3 Major Families of RL Algorithms<a href="#93-major-families-of-rl-algorithms" class="hash-link" aria-label="Direct link to 9.3 Major Families of RL Algorithms" title="Direct link to 9.3 Major Families of RL Algorithms" translate="no">â€‹</a></h2>
<p>There are many different RL algorithms, but most fall into three main families.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-value-based-methods-eg-q-learning">1. Value-Based Methods (e.g., Q-Learning)<a href="#1-value-based-methods-eg-q-learning" class="hash-link" aria-label="Direct link to 1. Value-Based Methods (e.g., Q-Learning)" title="Direct link to 1. Value-Based Methods (e.g., Q-Learning)" translate="no">â€‹</a></h3>
<p>These methods focus on learning a <strong>value function</strong>. A value function, <code>V(s)</code>, estimates the total future reward an agent can expect to get starting from state <code>s</code>. A related Q-function, <code>Q(s, a)</code>, estimates the value of taking action <code>a</code> in state <code>s</code>. The policy is then simple: in any given state, choose the action with the highest Q-value. These methods work very well for problems with a small, discrete set of actions, but are difficult to apply to the continuous action spaces of robotics.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-policy-based-methods-eg-reinforce">2. Policy-Based Methods (e.g., REINFORCE)<a href="#2-policy-based-methods-eg-reinforce" class="hash-link" aria-label="Direct link to 2. Policy-Based Methods (e.g., REINFORCE)" title="Direct link to 2. Policy-Based Methods (e.g., REINFORCE)" translate="no">â€‹</a></h3>
<p>Instead of learning a value function, these methods learn the policy directly. They treat the problem as an optimization problem, directly adjusting the parameters of the policy to increase the probability of taking actions that lead to high rewards. Policy-based methods are well-suited for robotics because they can handle continuous action spaces naturally.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-actor-critic-methods-eg-ppo-sac">3. Actor-Critic Methods (e.g., PPO, SAC)<a href="#3-actor-critic-methods-eg-ppo-sac" class="hash-link" aria-label="Direct link to 3. Actor-Critic Methods (e.g., PPO, SAC)" title="Direct link to 3. Actor-Critic Methods (e.g., PPO, SAC)" translate="no">â€‹</a></h3>
<p>Actor-Critic methods are a hybrid approach and represent the current state-of-the-art for most continuous control problems, including robotics. They use two neural networks:</p>
<ul>
<li class="">The <strong>Actor</strong> is the policy. It takes the current state and decides on an action.</li>
<li class="">The <strong>Critic</strong> is a value function. It observes the action taken by the Actor and evaluates how good that action was.</li>
</ul>
<p>The Actor is trying to get the highest score from the Critic, and the Critic is trying to provide an accurate evaluation. This interplay allows for much more stable and efficient learning than using either value-based or policy-based methods alone. <strong>Proximal Policy Optimization (PPO)</strong> and <strong>Soft Actor-Critic (SAC)</strong> are two of the most popular and successful actor-critic algorithms used in robotics today.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="94-challenges-of-rl-in-the-real-world">9.4 Challenges of RL in the Real World<a href="#94-challenges-of-rl-in-the-real-world" class="hash-link" aria-label="Direct link to 9.4 Challenges of RL in the Real World" title="Direct link to 9.4 Challenges of RL in the Real World" translate="no">â€‹</a></h2>
<p>While RL is powerful, applying it to real physical robots presents significant challenges:</p>
<ul>
<li class=""><strong>Sample Inefficiency:</strong> RL algorithms are notoriously &quot;data-hungry,&quot; often requiring millions or even billions of interactions with the environment to learn a good policy. This is simply not feasible on a real robot due to time and hardware wear-and-tear. This is the primary motivation for training in fast, parallelized simulators (<strong>Sim-to-Real</strong>).</li>
<li class=""><strong>Reward Function Design:</strong> The success of an RL agent is highly dependent on the design of its reward function. This is a delicate art. A poorly designed reward can lead to &quot;reward hacking,&quot; where the agent finds an unexpected way to maximize the reward without actually achieving the intended goal. For example, a humanoid rewarded for &quot;moving forward&quot; might learn that the fastest way to do so is to fall on its face.</li>
<li class=""><strong>Safety:</strong> The exploration phase of RL is inherently risky. A robot learning to walk will, by definition, fall down many times. A robot learning to use a tool might hit the table or itself. Ensuring safety for the robot and its environment during real-world training is a major open research problem.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/A-Siddiqui-coder/humanoid-robotics-book/tree/main/docs-site/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter9-reinforcement-learning/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter8-machine-learning/machine-learning-overview"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 8: Machine Learning for Robotics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter10-ros"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 10: Bipedal Locomotion: Walking and Balancing</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#91-the-language-of-reinforcement-learning" class="table-of-contents__link toc-highlight">9.1 The Language of Reinforcement Learning</a></li><li><a href="#92-exploration-vs-exploitation" class="table-of-contents__link toc-highlight">9.2 Exploration vs. Exploitation</a></li><li><a href="#93-major-families-of-rl-algorithms" class="table-of-contents__link toc-highlight">9.3 Major Families of RL Algorithms</a><ul><li><a href="#1-value-based-methods-eg-q-learning" class="table-of-contents__link toc-highlight">1. Value-Based Methods (e.g., Q-Learning)</a></li><li><a href="#2-policy-based-methods-eg-reinforce" class="table-of-contents__link toc-highlight">2. Policy-Based Methods (e.g., REINFORCE)</a></li><li><a href="#3-actor-critic-methods-eg-ppo-sac" class="table-of-contents__link toc-highlight">3. Actor-Critic Methods (e.g., PPO, SAC)</a></li></ul></li><li><a href="#94-challenges-of-rl-in-the-real-world" class="table-of-contents__link toc-highlight">9.4 Challenges of RL in the Real World</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book Sections</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part1-foundations/intro">Part 1: Foundations</a></li><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/intro">Part 2: Core Technologies</a></li><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part3-advanced-topics/intro">Part 3: Advanced Topics</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More Chapters</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part4-interdisciplinary-frontiers/intro">Part 4: Applications</a></li><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part1-foundations/chapter1-introduction">Chapter 1: Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part4-interdisciplinary-frontiers/chapter20-conclusion">Chapter 20: Conclusion</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Connect</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/A-Siddiqui-coder/humanoid-robotics-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Repository<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.governorsindh.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GIAIC Official<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Aziza Siddiqui | GIAIC Student. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>