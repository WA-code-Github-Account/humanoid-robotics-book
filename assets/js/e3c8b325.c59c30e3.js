"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[6036],{7702:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"physical-ai-humanoid-robotics/part2-core-technologies/index","title":"Part 2: Core Technologies","description":"Overview","source":"@site/docs/physical-ai-humanoid-robotics/part2-core-technologies/index.md","sourceDirName":"physical-ai-humanoid-robotics/part2-core-technologies","slug":"/physical-ai-humanoid-robotics/part2-core-technologies/","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/","draft":false,"unlisted":false,"editUrl":"https://github.com/A-Siddiqui-coder/humanoid-robotics-book/tree/main/docs-site/docs/physical-ai-humanoid-robotics/part2-core-technologies/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Dynamics and Control","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part1-foundations/chapter5-dynamics-control/dynamics-control-overview"},"next":{"title":"Chapter 6: Computer Vision for Object Recognition and Tracking","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter6-sensors-perception/"}}');var o=r(4848),t=r(8453);const s={},a="Part 2: Core Technologies",l={},c=[{value:"Overview",id:"overview",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Chapter 6: Sensors and Perception",id:"chapter-6-sensors-and-perception",level:3},{value:"Chapter 7: Computer Vision",id:"chapter-7-computer-vision",level:3},{value:"Chapter 8: Machine Learning",id:"chapter-8-machine-learning",level:3},{value:"Chapter 9: Reinforcement Learning",id:"chapter-9-reinforcement-learning",level:3},{value:"Chapter 10: ROS (Robot Operating System)",id:"chapter-10-ros-robot-operating-system",level:3},{value:"The Perception-Action Loop",id:"the-perception-action-loop",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Practical Focus",id:"practical-focus",level:2},{value:"Industry Relevance",id:"industry-relevance",level:2},{value:"Learning Path",id:"learning-path",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function h(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"part-2-core-technologies",children:"Part 2: Core Technologies"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Having established the foundations of humanoid robotics in Part 1, we now turn to the core technologies that enable robots to perceive, learn, and interact with their environment. This part bridges the gap between theoretical principles and practical implementation, introducing the sensors, algorithms, and software frameworks that power modern humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,o.jsx)(n.h3,{id:"chapter-6-sensors-and-perception",children:"Chapter 6: Sensors and Perception"}),"\n",(0,o.jsx)(n.p,{children:"Discover the sensory systems that give robots awareness of their surroundings\u2014from cameras and LiDAR to force/torque sensors and IMUs. Learn how raw sensor data is processed into meaningful environmental understanding."}),"\n",(0,o.jsx)(n.h3,{id:"chapter-7-computer-vision",children:"Chapter 7: Computer Vision"}),"\n",(0,o.jsx)(n.p,{children:'Explore how robots "see" the world through computer vision algorithms. Master object detection, tracking, semantic segmentation, and depth estimation techniques essential for robot navigation and manipulation.'}),"\n",(0,o.jsx)(n.h3,{id:"chapter-8-machine-learning",children:"Chapter 8: Machine Learning"}),"\n",(0,o.jsx)(n.p,{children:"Understand how robots learn from data using supervised, unsupervised, and self-supervised learning paradigms. Dive into neural network architectures specifically designed for robotic applications."}),"\n",(0,o.jsx)(n.h3,{id:"chapter-9-reinforcement-learning",children:"Chapter 9: Reinforcement Learning"}),"\n",(0,o.jsx)(n.p,{children:"Learn how robots discover optimal behaviors through trial and error. Explore policy gradient methods, value-based learning, and modern deep RL algorithms that enable robots to master complex tasks."}),"\n",(0,o.jsx)(n.h3,{id:"chapter-10-ros-robot-operating-system",children:"Chapter 10: ROS (Robot Operating System)"}),"\n",(0,o.jsx)(n.p,{children:"Master the de facto standard middleware for robotics development. Learn how ROS enables modular robot software architecture, inter-process communication, and integration of diverse hardware and algorithms."}),"\n",(0,o.jsx)(n.h2,{id:"the-perception-action-loop",children:"The Perception-Action Loop"}),"\n",(0,o.jsx)(n.p,{children:"At the heart of every intelligent robot is the perception-action loop:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sense"}),": Gather data from the environment through sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perceive"}),": Process sensor data to understand the world state"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decide"}),": Use learning algorithms to choose appropriate actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act"}),": Execute motor commands to affect the environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Learn"}),": Update internal models based on outcomes"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The technologies covered in this part implement each stage of this loop, transforming humanoid robots from mechanical sculptures into intelligent, adaptive systems."}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting Part 2, you should be comfortable with:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Mathematical foundations from Part 1"}),"\n",(0,o.jsx)(n.li,{children:"Python programming and NumPy"}),"\n",(0,o.jsx)(n.li,{children:"Basic understanding of robot kinematics and dynamics"}),"\n",(0,o.jsx)(n.li,{children:"Linear algebra and probability theory"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practical-focus",children:"Practical Focus"}),"\n",(0,o.jsx)(n.p,{children:"Unlike Part 1's theoretical emphasis, Part 2 is deeply practical. You'll work with:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Real sensor data and datasets"}),"\n",(0,o.jsx)(n.li,{children:"Popular machine learning frameworks (PyTorch, TensorFlow)"}),"\n",(0,o.jsx)(n.li,{children:"ROS simulation environments"}),"\n",(0,o.jsx)(n.li,{children:"Computer vision libraries (OpenCV, detectron2)"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each chapter includes hands-on code examples and exercises designed to build your implementation skills."}),"\n",(0,o.jsx)(n.h2,{id:"industry-relevance",children:"Industry Relevance"}),"\n",(0,o.jsx)(n.p,{children:"The technologies in this part are not just academic exercises\u2014they power today's commercial humanoid robots:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Boston Dynamics Atlas"}),": Computer vision for terrain perception and obstacle avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tesla Optimus"}),": Deep learning for vision-based manipulation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Agility Robotics Digit"}),": Reinforcement learning for adaptive locomotion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Figure 01"}),": Vision-language models for natural task understanding"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"By mastering these core technologies, you'll be equipped to contribute to cutting-edge robotics projects."}),"\n",(0,o.jsx)(n.h2,{id:"learning-path",children:"Learning Path"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Recommended Sequence"}),": Follow chapters 6\u21927\u21928\u21929\u219210 in order, as concepts build progressively."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"For Practitioners with ML Background"}),": You may skim Chapter 8 and focus more deeply on Chapters 7, 9, and 10 which are robotics-specific."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"For ROS Developers"}),": Chapter 10 will feel familiar, but earlier chapters provide essential context on the algorithms you'll integrate."]}),"\n",(0,o.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,o.jsx)(n.p,{children:"After completing Part 2, you'll understand how robots perceive and learn. Part 3 will synthesize these technologies into complete systems for locomotion, manipulation, and human-robot interaction."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Ready to explore sensors and perception? Turn to Chapter 6!"})})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);