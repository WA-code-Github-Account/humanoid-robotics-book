"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[9214],{4258:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/computer-vision-overview","title":"Chapter 7: Computer Vision","description":"Introduction","source":"@site/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/computer-vision-overview.md","sourceDirName":"physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision","slug":"/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/computer-vision-overview","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/computer-vision-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/A-Siddiqui-coder/humanoid-robotics-book/tree/main/docs-site/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/computer-vision-overview.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Reinforcement Learning for Robot Control","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter7-computer-vision/"},"next":{"title":"Chapter 8: Planning and Navigation Algorithms (SLAM)","permalink":"/humanoid-robotics-book/docs/physical-ai-humanoid-robotics/part2-core-technologies/chapter8-machine-learning/"}}');var o=n(4848),s=n(8453);const a={sidebar_position:7},r="Chapter 7: Computer Vision",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"7.1 Image Fundamentals",id:"71-image-fundamentals",level:2},{value:"7.2 The Deep Learning Revolution",id:"72-the-deep-learning-revolution",level:2},{value:"7.3 Core Vision Tasks in Robotics",id:"73-core-vision-tasks-in-robotics",level:2},{value:"7.4 3D Computer Vision",id:"74-3d-computer-vision",level:2}];function h(e){const i={em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"chapter-7-computer-vision",children:"Chapter 7: Computer Vision"})}),"\n",(0,o.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(i.p,{children:[(0,o.jsx)(i.strong,{children:"Computer vision"}),' is the science and technology of teaching machines to "see." It\'s a field of artificial intelligence that aims to extract meaningful information from images and videos, allowing a robot to understand the visual world. For a humanoid robot, vision is arguably the most important sense, providing a rich stream of data for navigating environments, avoiding obstacles, recognizing objects, and interacting with people.']}),"\n",(0,o.jsx)(i.p,{children:"This chapter dives into the core concepts of computer vision, from the classical techniques that laid the groundwork to the deep learning revolution that has enabled a new generation of powerful visual perception."}),"\n",(0,o.jsx)(i.h2,{id:"71-image-fundamentals",children:"7.1 Image Fundamentals"}),"\n",(0,o.jsxs)(i.p,{children:["An image, to a computer, is simply a large grid of numbers. Each number, called a ",(0,o.jsx)(i.strong,{children:"pixel"}),", represents the brightness or color at a specific point."]}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Color Spaces:"})," The most common color space is ",(0,o.jsx)(i.strong,{children:"RGB (Red, Green, Blue)"}),", where the color of each pixel is represented by three numbers. For many vision tasks, images are first converted to ",(0,o.jsx)(i.strong,{children:"grayscale"}),", where each pixel has only a single value representing its intensity from black to white."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Image Filtering:"})," A common first step in processing an image is to apply a filter to reduce noise. A ",(0,o.jsx)(i.strong,{children:"Gaussian blur"}),", for example, is a filter that averages each pixel with its neighbors, smoothing out the image and reducing random noise from the camera sensor."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Edge Detection:"})," Edges are one of the most fundamental features in an image. An ",(0,o.jsx)(i.strong,{children:"edge"})," is a curve that follows a path of rapid change in image intensity. Algorithms like the ",(0,o.jsx)(i.strong,{children:"Canny edge detector"}),' are highly effective at extracting a clean, thin set of edges from an image, providing a basic "line drawing" of the scene.']}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"72-the-deep-learning-revolution",children:"7.2 The Deep Learning Revolution"}),"\n",(0,o.jsxs)(i.p,{children:['For decades, computer vision relied on "hand-engineered" features. Researchers would design clever algorithms to detect specific patterns like corners, edges, and textures. A major breakthrough came with the rise of deep learning, and specifically, ',(0,o.jsx)(i.strong,{children:"Convolutional Neural Networks (CNNs)"}),"."]}),"\n",(0,o.jsx)(i.p,{children:'A CNN is a special type of neural network that learns to recognize patterns in images directly from data. It does this using layers of "convolutional filters."'}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Intuition behind CNNs:"})," Imagine the first layer of filters learns to recognize simple patterns like horizontal, vertical, and diagonal edges. The next layer can then combine these edge patterns to recognize more complex shapes like corners and curves. The next layer might combine those to recognize parts of objects, like an eye or a wheel. By stacking these layers, a CNN can learn a hierarchical representation of the visual world, all automatically from labeled training data."]}),"\n"]}),"\n",(0,o.jsx)(i.p,{children:"This ability to learn features automatically has led to superhuman performance on many vision tasks."}),"\n",(0,o.jsx)(i.h2,{id:"73-core-vision-tasks-in-robotics",children:"7.3 Core Vision Tasks in Robotics"}),"\n",(0,o.jsx)(i.p,{children:"Modern computer vision, powered by deep learning, allows a robot to perform several critical tasks:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Image Classification:"}),' This is the task of assigning a single label to an entire image (e.g., "This is a photo of a cat"). While simple, it\'s the foundation for many more complex tasks.']}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Object Detection:"}),' This task answers "What objects are in this image, and where are they?" The output is a set of ',(0,o.jsx)(i.strong,{children:"bounding boxes"}),' drawn around each detected object, along with a class label for each box (e.g., "person," "cup," "table"). This is essential for a robot that needs to find and interact with specific objects. Popular and effective object detection models include ',(0,o.jsx)(i.strong,{children:"YOLO (You Only Look Once)"}),"."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Semantic Segmentation:"})," This provides a much more detailed understanding of the scene. Instead of just drawing a box, semantic segmentation assigns a class label to ",(0,o.jsx)(i.em,{children:"every single pixel"}),' in the image. The output is an image that is "painted" with categories (e.g., all pixels belonging to people are colored blue, all pixels belonging to the road are colored gray). This is vital for navigation and scene understanding.']}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Instance Segmentation:"}),' This is a step beyond semantic segmentation. It not only labels each pixel but also distinguishes between different instances of the same object class (e.g., "person 1" is colored blue, and "person 2" is colored green).']}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"74-3d-computer-vision",children:"7.4 3D Computer Vision"}),"\n",(0,o.jsx)(i.p,{children:"A 2D image is a flat projection of a 3D world. For a robot to move and interact physically, it needs to recover the 3D structure of its environment."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Stereo Vision:"})," By using two cameras separated by a known distance (like human eyes), we can perceive depth. By identifying the same point in both the left and right images, we can measure the ",(0,o.jsx)(i.strong,{children:"disparity"})," (the difference in pixel location). A large disparity means the point is close, and a small disparity means it is far away. Using triangulation, this disparity can be converted into a depth value."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"RGB-D Cameras:"})," These specialized cameras (like the Microsoft Kinect or Intel RealSense) directly provide 3D information. They typically project an invisible pattern of infrared light into the scene and measure its distortion to calculate the distance to every pixel. The output is a ",(0,o.jsx)(i.strong,{children:"depth image"}),", where each pixel's value represents its distance from the camera. This technology greatly simplifies many 3D perception problems."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Visual SLAM:"})," As introduced in the previous chapter, SLAM (Simultaneous Localization and Mapping) is the process of building a map while simultaneously tracking the robot's location. ",(0,o.jsx)(i.strong,{children:"Visual SLAM"})," refers to systems that use one or more cameras as their primary sensor. These systems track visual features from frame to frame to estimate the camera's motion and build a 3D map of the environment, often as a sparse point cloud of features or a dense 3D reconstruction."]}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>r});var t=n(6540);const o={},s=t.createContext(o);function a(e){const i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);